{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Explainability with Alibi\n",
    "In this lab, you'll learn about the Alibi Explain library and implement global and local explanations of tabular and image classification models.\n",
    "\n",
    "Alibi Explain is an open source Python library aimed at machine learning model inspection and interpretation. The focus of the library is to provide high-quality implementations of black-box, white-box, local and global explanation methods for classification and regression models.\n",
    "\n",
    "Complete all the Deliverables mentioned below and show it to a TA for credit.\n",
    "\n",
    "## Deliverables\n",
    "- [ ] Finish all the TODOs in Section 1\n",
    "- [ ] Generate PD Plots for chosen features. Discuss your findings about model performance with the TA.\n",
    "- [ ] Play around with parameters for Anchor. Show final results to the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Clone this repository and run all cells in the notebook.\n",
    "\n",
    "### Install Dependencies\n",
    "\n",
    "For this assignment, make sure you have the required packages installed.\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "(If there are any major unsolvable issues prefer running this notebook on Google Colaboratory)\n",
    "\n",
    "### Possible Issues with installing Alibi\n",
    "`TypeError: issubclass() arg 1 must be a class` \\\n",
    "**Solution:** https://stackoverflow.com/questions/76313592/import-langchain-error-typeerror-issubclass-arg-1-must-be-a-class\n",
    "<br><br>\n",
    "If there's any more issues, please contact a TA to update this list (with a solution if its solved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from alibi.explainers import PartialDependence, plot_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Data + Modeling\n",
    "In this section, we will be setting up our data, perform some preprocessing and train a model. You will be getting the freedom of choosing your own dataset and training a custom machine learning model on it.\n",
    "\n",
    "**Note - Most of the code for this assignment is adapted from the Alibi example notebooks. Feel free to refer to their notebooks or official documentation for debugging (Links are in References section).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Fetching the Dataset\n",
    "Feel free to choose any tabular regression dataset, a good resource is the [Machine Learning with R Datasets GitHub](https://github.com/stedy/Machine-Learning-with-R-datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Load your Dataset\n",
    "df = pd.read_csv('PLACEHOLDER.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset Preprocessing\n",
    "Finish all the TODOs in this section as per your dataset, this should be straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature names\n",
    "feature_names = df.columns.tolist()\n",
    "# TODO - remove the target feature from feature names\n",
    "feature_names.remove('FEATURE_NAME')\n",
    "\n",
    "# TODO - define target names\n",
    "target_names = ['TARGET_FEATURE']\n",
    "\n",
    "# TODO - define categorical columns\n",
    "categorical_columns_names = ['FEATURE_LIST']\n",
    "\n",
    "# define categorical and numerical indices for later preprocessing\n",
    "categorical_columns_indices = [feature_names.index(cn) for cn in categorical_columns_names]\n",
    "numerical_columns_indices = [feature_names.index(fn) for fn in feature_names if fn not in categorical_columns_names]\n",
    "\n",
    "# extract data\n",
    "X = df[feature_names]\n",
    "# TODO - define target column\n",
    "y = df['TARGET_COLUMN']\n",
    "\n",
    "# split data in train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit the oridnal encoder\n",
    "oe = OrdinalEncoder().fit(X_train[categorical_columns_names])\n",
    "\n",
    "# transform the categorical columns to ordinal encoding\n",
    "X_train.loc[:, categorical_columns_names] = oe.transform(X_train[categorical_columns_names])\n",
    "X_test.loc[:, categorical_columns_names] = oe.transform(X_test[categorical_columns_names])\n",
    "\n",
    "# convert data to numpy\n",
    "X_train, y_train = X_train.to_numpy(), y_train.to_numpy()\n",
    "X_test, y_test = X_test.to_numpy(), y_test.to_numpy()\n",
    "\n",
    "# define categorical mappings\n",
    "categorical_names = {i: list(v) for (i, v) in zip(categorical_columns_indices, oe.categories_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPT] Print this variable to see different classes of categorical names\n",
    "print(categorical_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define numerical standard sclaer\n",
    "num_transf = StandardScaler()\n",
    "\n",
    "# define categorical one-hot encoder\n",
    "cat_transf = OneHotEncoder(\n",
    "    categories=[range(len(x)) for x in categorical_names.values()],\n",
    "    handle_unknown='ignore',\n",
    ")\n",
    "\n",
    "# define preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', cat_transf, categorical_columns_indices),\n",
    "        ('num', num_transf, numerical_columns_indices),\n",
    "    ],\n",
    "    sparse_threshold=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# preprocess train and test datasets\n",
    "X_train_ohe = preprocessor.transform(X_train)\n",
    "X_test_ohe = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Training the Model\n",
    "Run the cell to fit the model to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit regressor - feel free to play with the hyperparameters\n",
    "predictor = RandomForestRegressor(random_state=0)\n",
    "predictor.fit(X_train_ohe, y_train)\n",
    "\n",
    "# compute scores\n",
    "print('Train score: %.2f' % (predictor.score(X_train_ohe, y_train)))\n",
    "print('Test score: %.2f' % (predictor.score(X_test_ohe, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Explainability with Alibi\n",
    "In this section, we will be finally using Alibi to explain our trained model using different techniques. Fill in the TODOs and generate plots where required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Partial Dependence (PD)\n",
    "\n",
    "Partial Dependence is a method to visualize the marginal effect that one or two features have on the predicted outcome of a machine learning model.\n",
    "\n",
    "By inspecting the PD plots, one can understand whether the relation between a feature/pair of features is, for example, a simple linear or quadratic relation, whether it presents a monotonically increasing or decreasing trend, or reveal a more complex response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Prediction Function - Includes pipeline from preprocessing to prediction\n",
    "prediction_fn = lambda x: predictor.predict(preprocessor.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define explainer\n",
    "explainer = PartialDependence(predictor=prediction_fn,\n",
    "                       feature_names=feature_names,\n",
    "                       categorical_names=categorical_names,\n",
    "                       target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Select features you wish to compute explanations for\n",
    "# Hint - Select Column Indices = feature_names.index(\"COLUMN_NAME\")\n",
    "features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Explanations\n",
    "We will finally be computing explanations for selected features. Play around with different modes of `kind` to see different visualizations.\n",
    "\n",
    "As per the documentation,\n",
    "> `kind` - If set to `average`, then only the partial dependence (PD) averaged across all samples from the dataset is returned. If set to `individual`, then only the individual conditional expectation (ICE) is returned for each individual from the dataset. Otherwise, if set to `both`, then both the PD and the ICE are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute explanations\n",
    "exp = explainer.explain(X=X_train,\n",
    "                        features=features,\n",
    "                        kind='both') # kind = [both, individual, average]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot partial dependece curves\n",
    "# this setup should work for any mode of kind, feel free to alter this to better suit your plot view.\n",
    "plot_pd(exp=exp,\n",
    "        n_cols=3,\n",
    "        n_ice=50,\n",
    "        sharey='row',\n",
    "        center = True,\n",
    "        fig_kw={'figheight': 10, 'figwidth': 15});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Partial Dependence for Two Features\n",
    "Choose pairs of features to visualize their relationships and interactions with each other. These plots may be a bit confusing so try your best to explain whatever you understand to the TA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select combination of features\n",
    "combined_features = [(feature_names.index('FEATURE1'), feature_names.index('FEATURE2'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute explanations\n",
    "comb_exp = explainer.explain(X=X_train,\n",
    "                        features=combined_features,\n",
    "                        kind='average',\n",
    "                        grid_resolution=25) # kind = [both, individual, average]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot partial dependece curves\n",
    "plot_pd(exp=comb_exp,\n",
    "        n_cols=3,\n",
    "        n_ice=50,\n",
    "        sharey='row',\n",
    "        center = True,\n",
    "        fig_kw={'figheight': 10, 'figwidth': 15});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Anchors\n",
    "This algorithm provides model-agnostic (black box) and human interpretable explanations suitable for classification models applied to images, text and tabular data. The idea behind anchors is to explain the behaviour of complex models with high-precision rules called anchors. These anchors are locally sufficient conditions to ensure a certain prediction with a high degree of confidence. Run all the cells and play around with parameters to see how this algorithm works.\n",
    "\n",
    "For this example, we will be explaining predictions from the ImageNet model on the cats dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
    "from alibi.datasets import load_cats\n",
    "from alibi.explainers import AnchorImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionV3(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (299, 299, 3)\n",
    "data, labels = load_cats(target_size=image_shape[:2], return_X_y=True)\n",
    "print(f'Images shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = preprocess_input(data)\n",
    "preds = model.predict(images)\n",
    "label = decode_predictions(preds, top=3)\n",
    "print(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction function\n",
    "predict_fn = lambda x: model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_fn = 'slic' # choose from different segmentation functions - https://scikit-image.org/docs/dev/api/skimage.segmentation.html\n",
    "kwargs = {'n_segments': 15, 'compactness': 20, 'sigma': .5, 'start_label': 0} # play around with parameters\n",
    "explainer = AnchorImage(predict_fn, image_shape, segmentation_fn=segmentation_fn,\n",
    "                        segmentation_kwargs=kwargs, images_background=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # you can change i to choose another picture\n",
    "plt.imshow(data[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = images[i]\n",
    "np.random.seed(0)\n",
    "explanation = explainer.explain(image, threshold=.95, p_sample=.5, tau=0.25) # play around with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(explanation.anchor);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(explanation.segments);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. https://docs.seldon.io/projects/alibi/en/stable/examples/pdp_regression_bike.html\n",
    "2. https://docs.seldon.io/projects/alibi/en/stable/methods/PartialDependence.html\n",
    "3. https://docs.seldon.io/projects/alibi/en/stable/examples/anchor_image_imagenet.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
